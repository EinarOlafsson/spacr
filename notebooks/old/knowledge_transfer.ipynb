{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec23232c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9346e12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "def model_knowledge_transfer(settings):\n",
    "    \"\"\"\n",
    "    High-level wrapper that:\n",
    "      1) Generates a training DataLoader from 'src' using generate_loaders.\n",
    "      2) Performs knowledge distillation from multiple teacher models onto a \n",
    "         single student, using the new labeled data for both supervised CE \n",
    "         and distillation loss.\n",
    "      3) Saves the distilled student to `student_save_path`.\n",
    "\n",
    "    Args:\n",
    "        teacher_paths (list[str]): Paths to teacher checkpoints (TorchModel or dict).\n",
    "        src (str): Source directory for your new labeled data (with subfolders 'train'/'test').\n",
    "        student_save_path (str): Where to save the final student TorchModel.\n",
    "        device (str): 'cpu' or 'cuda'.\n",
    "        student_model_name, pretrained, dropout_rate, use_checkpoint: \n",
    "            TorchModel init arguments for the student.\n",
    "        alpha, temperature, lr, epochs: Distillation hyperparams.\n",
    "        image_size, batch_size, classes, etc.: Passed to generate_loaders.\n",
    "    \"\"\"\n",
    "\n",
    "    from spacr.io import generate_loaders\n",
    "    \n",
    "    def _knowledge_transfer(\n",
    "        teacher_paths,\n",
    "        student_save_path,\n",
    "        data_loader,            # DataLoader for (images, labels)\n",
    "        device='cpu',\n",
    "        student_model_name='maxvit_t',\n",
    "        pretrained=True,\n",
    "        dropout_rate=None,\n",
    "        use_checkpoint=False,\n",
    "        alpha=0.5,\n",
    "        temperature=2.0,\n",
    "        lr=1e-4,\n",
    "        epochs=10\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Performs multi-teacher knowledge distillation on a new labeled dataset,\n",
    "        producing a single student TorchModel that combines the teachers' knowledge\n",
    "        plus the labeled data.\n",
    "\n",
    "        Args:\n",
    "            teacher_paths (list[str]): Paths to teacher models (TorchModel or dict).\n",
    "            student_save_path (str): Destination path to save the final student.\n",
    "            data_loader (DataLoader): Yields (images, labels) from the new dataset.\n",
    "            device (str): 'cpu' or 'cuda'.\n",
    "            student_model_name (str): Architecture name for the student TorchModel.\n",
    "            pretrained (bool): If the student should be initialized as pretrained.\n",
    "            dropout_rate (float): If needed by your TorchModel init.\n",
    "            use_checkpoint (bool): If needed by your TorchModel init.\n",
    "            alpha (float): Weight for real-label CE vs. distillation loss.\n",
    "            temperature (float): Distillation temperature.\n",
    "            lr (float): Learning rate for the student.\n",
    "            epochs (int): Number of training epochs.\n",
    "\n",
    "        Returns:\n",
    "            TorchModel: The final, trained student model.\n",
    "        \"\"\"\n",
    "        from spacr.utils import TorchModel  # adjust if needed\n",
    "\n",
    "        # Adjust filename to reflect KD if desired\n",
    "        import os\n",
    "        base, ext = os.path.splitext(student_save_path)\n",
    "        if not ext:\n",
    "            ext = '.pth'\n",
    "        student_save_path = f\"{base}_KD{ext}\"\n",
    "\n",
    "        # 1) Load teacher models\n",
    "        teachers = []\n",
    "        print(\"Loading teacher models:\")\n",
    "        for path in teacher_paths:\n",
    "            print(f\"  Loading teacher: {path}\")\n",
    "            ckpt = torch.load(path, map_location=device)\n",
    "            if isinstance(ckpt, TorchModel):\n",
    "                teacher = ckpt.to(device)\n",
    "            elif isinstance(ckpt, dict):\n",
    "                from spacr.utils import TorchModel\n",
    "                teacher = TorchModel(\n",
    "                    model_name=ckpt.get('model_name', student_model_name),\n",
    "                    pretrained=ckpt.get('pretrained', pretrained),\n",
    "                    dropout_rate=ckpt.get('dropout_rate', dropout_rate),\n",
    "                    use_checkpoint=ckpt.get('use_checkpoint', use_checkpoint)\n",
    "                ).to(device)\n",
    "                teacher.load_state_dict(ckpt['model'])\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported checkpoint type at {path} (must be TorchModel or dict).\")\n",
    "\n",
    "            teacher.eval()  # for consistent BN, dropout\n",
    "            teachers.append(teacher)\n",
    "\n",
    "        # 2) Initialize the student TorchModel\n",
    "        student_model = TorchModel(\n",
    "            model_name=student_model_name,\n",
    "            pretrained=pretrained,\n",
    "            dropout_rate=dropout_rate,\n",
    "            use_checkpoint=use_checkpoint\n",
    "        ).to(device)\n",
    "\n",
    "        # 3) Optimizer\n",
    "        optimizer = optim.Adam(student_model.parameters(), lr=lr)\n",
    "\n",
    "        # 4) Training loop\n",
    "        for epoch in range(epochs):\n",
    "            student_model.train()\n",
    "            running_loss = 0.0\n",
    "\n",
    "            for images, labels in data_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Forward pass: student\n",
    "                logits_s = student_model(images)\n",
    "                logits_s_temp = logits_s / temperature\n",
    "\n",
    "                # Distillation: get average teacher probabilities\n",
    "                with torch.no_grad():\n",
    "                    teacher_probs_list = []\n",
    "                    for tm in teachers:\n",
    "                        logits_t = tm(images) / temperature\n",
    "                        teacher_probs_list.append(F.softmax(logits_t, dim=1))\n",
    "                    teacher_probs_ensemble = torch.mean(torch.stack(teacher_probs_list), dim=0)\n",
    "\n",
    "                # Student distribution\n",
    "                student_log_probs = F.log_softmax(logits_s_temp, dim=1)\n",
    "\n",
    "                # Distillation loss (KLDiv)\n",
    "                loss_distill = F.kl_div(\n",
    "                    student_log_probs,\n",
    "                    teacher_probs_ensemble,\n",
    "                    reduction='batchmean'\n",
    "                ) * (temperature ** 2)\n",
    "\n",
    "                # Supervised loss (CE with real labels)\n",
    "                loss_ce = F.cross_entropy(logits_s, labels)\n",
    "\n",
    "                # Weighted sum\n",
    "                loss = alpha * loss_ce + (1 - alpha) * loss_distill\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            avg_loss = running_loss / len(data_loader)\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # 5) Save final student\n",
    "        torch.save(student_model, student_save_path)\n",
    "        print(f\"Knowledge-distilled student saved to: {student_save_path}\")\n",
    "\n",
    "        return student_model\n",
    "    \n",
    "    # 1) Generate DataLoader\n",
    "    print(\"Generating training DataLoader...\")\n",
    "    train_loaders, val_loaders, train_fig = generate_loaders(\n",
    "        src=src,\n",
    "        mode='train',\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        classes=classes,\n",
    "        n_jobs=n_jobs,\n",
    "        validation_split=validation_split,\n",
    "        pin_memory=pin_memory,\n",
    "        normalize=normalize,\n",
    "        channels=channels,\n",
    "        augment=augment,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    # If validation_split=0, train_loaders is a single DataLoader\n",
    "    # If >0, it's a DataLoader for train, and val_loaders is for validation\n",
    "    if validation_split > 0.0:\n",
    "        print(\"Note: We'll only use the train DataLoader for knowledge distillation, ignoring val_loaders.\")\n",
    "        train_loader = train_loaders\n",
    "    else:\n",
    "        train_loader = train_loaders  # or whichever you used\n",
    "\n",
    "    # 2) Perform knowledge distillation\n",
    "    #from .my_code import model_knowledge_transfer  # or your actual import\n",
    "    distilled_student = _knowledge_transfer(\n",
    "        teacher_paths=teacher_paths,\n",
    "        student_save_path=student_save_path,\n",
    "        data_loader=train_loader,\n",
    "        device=device,\n",
    "        student_model_name=student_model_name,\n",
    "        pretrained=pretrained,\n",
    "        dropout_rate=dropout_rate,\n",
    "        use_checkpoint=use_checkpoint,\n",
    "        alpha=alpha,\n",
    "        temperature=temperature,\n",
    "        lr=lr,\n",
    "        epochs=epochs\n",
    "    )\n",
    "\n",
    "    print(\"Distillation complete. Student model returned.\")\n",
    "    return distilled_student, train_fig\n",
    "\n",
    "teacher_paths = ['/nas_mnt/carruthers/Einar/tsg101_screen/TSG101SCREEN_20240810_132824/plate1/datasets/training/model/maxvit_t/rgb/epochs_100/maxvit_t_epoch_100_channels_rgb.pth',\n",
    "              '/nas_mnt/carruthers/Einar/tsg101_screen/TSG101SCREEN_20240824_072829/plate2/datasets/training/model/maxvit_t/rgb/epochs_100/maxvit_t_epoch_100_channels_rgb.pth',\n",
    "              '/nas_mnt/carruthers/Einar/tsg101_screen/TSG101SCREEN_20240825_094106/plate3/datasets/training/model/maxvit_t/rgb/epochs_100/maxvit_t_epoch_100_channels_rgb.pth',\n",
    "              '/nas_mnt/carruthers/Einar/tsg101_screen/TSG101SCREEN_20240826_140251/plate4/datasets/training/model/maxvit_t/rgb/epochs_100/maxvit_t_epoch_100_channels_rgb.pth']\n",
    "\n",
    "src = '/nas_mnt/carruthers/Einar/tsg101_screen/hits_20250108_181547/plate2/datasets/training'\n",
    "\n",
    "settings = {\n",
    "    'teacher_paths':['list of paths'],\n",
    "    'src':'path',\n",
    "    'student_save_path':'save_path',\n",
    "    'device':'cpu',\n",
    "    'student_model_name':'maxvit_t',\n",
    "    'pretrained':True,\n",
    "    'dropout_rate':None,\n",
    "    'use_checkpoint':False,\n",
    "    'alpha':0.5,\n",
    "    'temperature':2.0,\n",
    "    'lr':1e-4,\n",
    "    'epochs':10,\n",
    "    'image_size':224,\n",
    "    'batch_size':32,\n",
    "    'classes':('nc','pc'),\n",
    "    'n_jobs':None,\n",
    "    'validation_split':0.0,\n",
    "    'pin_memory':False,\n",
    "    'normalize':False,\n",
    "    'channels':('r','g','b'),\n",
    "    'augment':False,\n",
    "    'verbose':False}\n",
    "\n",
    "model_knowledge_transfer(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d36d5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating training DataLoader(s) from: /nas_mnt/carruthers/Einar/tsg101_screen/hits_20250108_181547/plate2/datasets/training\n",
      "Loading Train and validation datasets\n",
      "Loading teacher models:\n",
      "  Loading teacher: /nas_mnt/carruthers/Einar/tsg101_screen/TSG101SCREEN_20240810_132824/plate1/datasets/training/model/maxvit_t/rgb/epochs_100/maxvit_t_epoch_100_channels_rgb.pth\n",
      "  Loading teacher: /nas_mnt/carruthers/Einar/tsg101_screen/TSG101SCREEN_20240824_072829/plate2/datasets/training/model/maxvit_t/rgb/epochs_100/maxvit_t_epoch_100_channels_rgb.pth\n",
      "  Loading teacher: /nas_mnt/carruthers/Einar/tsg101_screen/TSG101SCREEN_20240825_094106/plate3/datasets/training/model/maxvit_t/rgb/epochs_100/maxvit_t_epoch_100_channels_rgb.pth\n",
      "  Loading teacher: /nas_mnt/carruthers/Einar/tsg101_screen/TSG101SCREEN_20240826_140251/plate4/datasets/training/model/maxvit_t/rgb/epochs_100/maxvit_t_epoch_100_channels_rgb.pth\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 262\u001b[0m\n\u001b[1;32m    236\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/nas_mnt/carruthers/Einar/tsg101_screen/hits_20250108_181547/plate2/datasets/training\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    238\u001b[0m settings \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mteacher_paths\u001b[39m\u001b[38;5;124m'\u001b[39m:teacher_paths,\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrc\u001b[39m\u001b[38;5;124m'\u001b[39m:src,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maugment\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;28;01mFalse\u001b[39;00m}\n\u001b[0;32m--> 262\u001b[0m distilled_model, train_fig \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_knowledge_transfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msettings\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 212\u001b[0m, in \u001b[0;36mmodel_knowledge_transfer\u001b[0;34m(settings)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m student_model\n\u001b[1;32m    211\u001b[0m \u001b[38;5;66;03m# -- 3) Perform knowledge distillation using the internal function --\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m distilled_student \u001b[38;5;241m=\u001b[39m \u001b[43m_knowledge_transfer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mteacher_paths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mteacher_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstudent_save_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstudent_save_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstudent_model_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstudent_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDistillation complete. Student model returned.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m distilled_student, train_fig\n",
      "Cell \u001b[0;32mIn[16], line 165\u001b[0m, in \u001b[0;36mmodel_knowledge_transfer.<locals>._knowledge_transfer\u001b[0;34m(teacher_paths, student_save_path, data_loader, device, student_model_name, pretrained, dropout_rate, use_checkpoint, alpha, temperature, lr, epochs)\u001b[0m\n\u001b[1;32m    162\u001b[0m student_model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    163\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[1;32m    166\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    167\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "def model_knowledge_transfer(settings):\n",
    "    \"\"\"\n",
    "    High-level wrapper that:\n",
    "      1) Generates a training DataLoader from 'src' using generate_loaders.\n",
    "      2) Performs knowledge distillation from multiple teacher models onto a \n",
    "         single student TorchModel, using the new labeled data for both \n",
    "         supervised CE and distillation loss.\n",
    "      3) Saves the distilled student to `student_save_path`.\n",
    "\n",
    "    Args:\n",
    "        settings (dict): A dictionary containing all the necessary parameters:\n",
    "            {\n",
    "              \"teacher_paths\": list of str,  # Paths to teacher checkpoints\n",
    "              \"src\": str,                    # Source directory for new labeled data\n",
    "              \"student_save_path\": str,      # Where to save the final student model\n",
    "              \"device\": str,                 # 'cpu' or 'cuda'\n",
    "              \"student_model_name\": str,     # e.g. 'maxvit_t'\n",
    "              \"pretrained\": bool,\n",
    "              \"dropout_rate\": float or None,\n",
    "              \"use_checkpoint\": bool,\n",
    "              \"alpha\": float,                # Weight for real-label CE vs. distillation\n",
    "              \"temperature\": float,          # Distillation temperature\n",
    "              \"lr\": float,                   # Learning rate\n",
    "              \"epochs\": int,                 # Training epochs\n",
    "              \"image_size\": int,\n",
    "              \"batch_size\": int,\n",
    "              \"classes\": tuple,              # e.g. ('nc','pc')\n",
    "              \"n_jobs\": int or None,\n",
    "              \"validation_split\": float,\n",
    "              \"pin_memory\": bool,\n",
    "              \"normalize\": bool,\n",
    "              \"channels\": tuple,             # e.g. ('r','g','b')\n",
    "              \"augment\": bool,\n",
    "              \"verbose\": bool\n",
    "            }\n",
    "\n",
    "    Returns:\n",
    "        (student_model, train_fig):\n",
    "            student_model: The final, trained (distilled) TorchModel\n",
    "            train_fig:     A figure from generate_loaders() if any (or None)\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract arguments from the settings dict, with defaults or fallback\n",
    "    teacher_paths      = settings.get('teacher_paths', [])\n",
    "    src                = settings.get('src', '')\n",
    "    student_save_path  = settings.get('student_save_path', 'distilled_student.pth')\n",
    "    device             = settings.get('device', 'cpu')\n",
    "    student_model_name = settings.get('student_model_name', 'maxvit_t')\n",
    "    pretrained         = settings.get('pretrained', True)\n",
    "    dropout_rate       = settings.get('dropout_rate', None)\n",
    "    use_checkpoint     = settings.get('use_checkpoint', False)\n",
    "    alpha              = settings.get('alpha', 0.5)\n",
    "    temperature        = settings.get('temperature', 2.0)\n",
    "    lr                 = settings.get('lr', 1e-4)\n",
    "    epochs             = settings.get('epochs', 10)\n",
    "    image_size         = settings.get('image_size', 224)\n",
    "    batch_size         = settings.get('batch_size', 32)\n",
    "    classes            = settings.get('classes', ('nc', 'pc'))\n",
    "    n_jobs             = settings.get('n_jobs', None)\n",
    "    validation_split   = settings.get('validation_split', 0.0)\n",
    "    pin_memory         = settings.get('pin_memory', False)\n",
    "    normalize          = settings.get('normalize', False)\n",
    "    channels           = settings.get('channels', ('r','g','b'))\n",
    "    augment            = settings.get('augment', False)\n",
    "    verbose            = settings.get('verbose', False)\n",
    "\n",
    "    # -- 1) generate_loaders (returns train_loaders, val_loaders, train_fig) --\n",
    "    from spacr.io import generate_loaders\n",
    "    print(\"Generating training DataLoader(s) from:\", src)\n",
    "    train_loaders, val_loaders, train_fig = generate_loaders(\n",
    "        src=src,\n",
    "        mode='train',\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        classes=classes,\n",
    "        n_jobs=n_jobs,\n",
    "        validation_split=validation_split,\n",
    "        pin_memory=pin_memory,\n",
    "        normalize=normalize,\n",
    "        channels=channels,\n",
    "        augment=augment,\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    # If validation_split > 0, train_loaders is the train set, val_loaders is val set\n",
    "    # Otherwise train_loaders is a single DataLoader.\n",
    "    if validation_split > 0.0:\n",
    "        print(\"Note: We'll only use the train_loader for knowledge distillation, ignoring val_loader.\")\n",
    "        train_loader = train_loaders\n",
    "    else:\n",
    "        train_loader = train_loaders\n",
    "\n",
    "    # -- 2) define the internal knowledge-distillation function --\n",
    "    def _knowledge_transfer(\n",
    "        teacher_paths,\n",
    "        student_save_path,\n",
    "        data_loader,\n",
    "        device='cpu',\n",
    "        student_model_name='maxvit_t',\n",
    "        pretrained=True,\n",
    "        dropout_rate=None,\n",
    "        use_checkpoint=False,\n",
    "        alpha=0.5,\n",
    "        temperature=2.0,\n",
    "        lr=1e-4,\n",
    "        epochs=10\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Performs multi-teacher knowledge distillation on a new labeled dataset,\n",
    "        producing a single student TorchModel that combines the teachers' knowledge\n",
    "        plus the labeled data.\n",
    "        \"\"\"\n",
    "        from spacr.utils import TorchModel  # or wherever TorchModel is located\n",
    "\n",
    "        # Adjust filename to reflect KD if desired\n",
    "        base, ext = os.path.splitext(student_save_path)\n",
    "        if not ext:\n",
    "            ext = '.pth'\n",
    "        student_save_path = f\"{base}_KD{ext}\"\n",
    "\n",
    "        # 1) Load teacher models\n",
    "        teachers = []\n",
    "        print(\"Loading teacher models:\")\n",
    "        for path in teacher_paths:\n",
    "            print(f\"  Loading teacher: {path}\")\n",
    "            ckpt = torch.load(path, map_location=device)\n",
    "            if isinstance(ckpt, TorchModel):\n",
    "                teacher = ckpt.to(device)\n",
    "            elif isinstance(ckpt, dict):\n",
    "                # create a new TorchModel with possible metadata\n",
    "                teacher = TorchModel(\n",
    "                    model_name=ckpt.get('model_name', student_model_name),\n",
    "                    pretrained=ckpt.get('pretrained', pretrained),\n",
    "                    dropout_rate=ckpt.get('dropout_rate', dropout_rate),\n",
    "                    use_checkpoint=ckpt.get('use_checkpoint', use_checkpoint)\n",
    "                ).to(device)\n",
    "                teacher.load_state_dict(ckpt['model'])\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported checkpoint type at {path} (must be TorchModel or dict).\")\n",
    "\n",
    "            teacher.eval()  # freeze teacher in eval mode\n",
    "            teachers.append(teacher)\n",
    "\n",
    "        # 2) Initialize the student TorchModel\n",
    "        student_model = TorchModel(\n",
    "            model_name=student_model_name,\n",
    "            pretrained=pretrained,\n",
    "            dropout_rate=dropout_rate,\n",
    "            use_checkpoint=use_checkpoint\n",
    "        ).to(device)\n",
    "\n",
    "        # 3) Setup optimizer\n",
    "        optimizer = optim.Adam(student_model.parameters(), lr=lr)\n",
    "\n",
    "        # 4) Distillation training loop\n",
    "        for epoch in range(epochs):\n",
    "            student_model.train()\n",
    "            running_loss = 0.0\n",
    "\n",
    "            for images, labels in data_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Forward pass (student)\n",
    "                logits_s = student_model(images)\n",
    "                logits_s_temp = logits_s / temperature\n",
    "\n",
    "                # Teacher ensemble output\n",
    "                with torch.no_grad():\n",
    "                    teacher_probs_list = []\n",
    "                    for tm in teachers:\n",
    "                        logits_t = tm(images) / temperature\n",
    "                        teacher_probs_list.append(F.softmax(logits_t, dim=1))\n",
    "                    teacher_probs_ensemble = torch.mean(torch.stack(teacher_probs_list), dim=0)\n",
    "\n",
    "                student_log_probs = F.log_softmax(logits_s_temp, dim=1)\n",
    "\n",
    "                # Distillation loss\n",
    "                loss_distill = F.kl_div(\n",
    "                    student_log_probs,\n",
    "                    teacher_probs_ensemble,\n",
    "                    reduction='batchmean'\n",
    "                ) * (temperature ** 2)\n",
    "\n",
    "                # Supervised CE with ground-truth labels\n",
    "                loss_ce = F.cross_entropy(logits_s, labels)\n",
    "\n",
    "                # Weighted total loss\n",
    "                loss = alpha * loss_ce + (1 - alpha) * loss_distill\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            avg_loss = running_loss / len(data_loader)\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # 5) Save the final student\n",
    "        torch.save(student_model, student_save_path)\n",
    "        print(f\"Knowledge-distilled student saved to: {student_save_path}\")\n",
    "\n",
    "        return student_model\n",
    "\n",
    "    # -- 3) Perform knowledge distillation using the internal function --\n",
    "    distilled_student = _knowledge_transfer(\n",
    "        teacher_paths=teacher_paths,\n",
    "        student_save_path=student_save_path,\n",
    "        data_loader=train_loader,\n",
    "        device=device,\n",
    "        student_model_name=student_model_name,\n",
    "        pretrained=pretrained,\n",
    "        dropout_rate=dropout_rate,\n",
    "        use_checkpoint=use_checkpoint,\n",
    "        alpha=alpha,\n",
    "        temperature=temperature,\n",
    "        lr=lr,\n",
    "        epochs=epochs\n",
    "    )\n",
    "\n",
    "    print(\"Distillation complete. Student model returned.\")\n",
    "    return distilled_student, train_fig\n",
    "\n",
    "\n",
    "teacher_paths = ['/nas_mnt/carruthers/Einar/tsg101_screen/TSG101SCREEN_20240810_132824/plate1/datasets/training/model/maxvit_t/rgb/epochs_100/maxvit_t_epoch_100_channels_rgb.pth',\n",
    "              '/nas_mnt/carruthers/Einar/tsg101_screen/TSG101SCREEN_20240824_072829/plate2/datasets/training/model/maxvit_t/rgb/epochs_100/maxvit_t_epoch_100_channels_rgb.pth',\n",
    "              '/nas_mnt/carruthers/Einar/tsg101_screen/TSG101SCREEN_20240825_094106/plate3/datasets/training/model/maxvit_t/rgb/epochs_100/maxvit_t_epoch_100_channels_rgb.pth',\n",
    "              '/nas_mnt/carruthers/Einar/tsg101_screen/TSG101SCREEN_20240826_140251/plate4/datasets/training/model/maxvit_t/rgb/epochs_100/maxvit_t_epoch_100_channels_rgb.pth']\n",
    "\n",
    "src = '/nas_mnt/carruthers/Einar/tsg101_screen/hits_20250108_181547/plate2/datasets/training'\n",
    "\n",
    "settings = {\n",
    "    'teacher_paths':teacher_paths,\n",
    "    'src':src,\n",
    "    'student_save_path':'/nas_mnt/carruthers/Einar/tsg101_screen/hits_20250108_181547/plate2/kt_model',\n",
    "    'device':'cpu',\n",
    "    'student_model_name':'maxvit_t',\n",
    "    'pretrained':True,\n",
    "    'dropout_rate':None,\n",
    "    'use_checkpoint':False,\n",
    "    'alpha':0.5,\n",
    "    'temperature':2.0,\n",
    "    'lr':1e-4,\n",
    "    'epochs':10,\n",
    "    'image_size':224,\n",
    "    'batch_size':64,\n",
    "    'classes':('nc','pc'),\n",
    "    'n_jobs':None,\n",
    "    'validation_split':0.0,\n",
    "    'pin_memory':False,\n",
    "    'normalize':False,\n",
    "    'channels':('r','g','b'),\n",
    "    'augment':False,\n",
    "    'verbose':False}\n",
    "\n",
    "distilled_model, train_fig = model_knowledge_transfer(settings)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacr",
   "language": "python",
   "name": "spacr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
