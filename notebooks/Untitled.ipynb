{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b2a336",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPNN(MessagePassing):\n",
    "    def __init__(self, node_in_features, edge_in_features, out_features):\n",
    "        super(MPNN, self).__init__(aggr='mean')  # 'mean' aggregation.\n",
    "        self.message_mlp = Sequential(\n",
    "            Linear(node_in_features + edge_in_features, 128),\n",
    "            ReLU(),\n",
    "            Linear(128, out_features)\n",
    "        )\n",
    "        self.update_mlp = Sequential(\n",
    "            Linear(out_features, out_features),\n",
    "            ReLU(),\n",
    "            Linear(out_features, out_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # x: Node features [N, node_in_features]\n",
    "        # edge_index: Graph connectivity [2, E]\n",
    "        # edge_attr: Edge attributes/features [E, edge_in_features]\n",
    "        return self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "\n",
    "    def message(self, x_j, edge_attr):\n",
    "        # x_j: Input features of neighbors [E, node_in_features]\n",
    "        # edge_attr: Edge attributes [E, edge_in_features]\n",
    "        tmp = torch.cat([x_j, edge_attr], dim=-1)  # Concatenate node features with edge attributes\n",
    "        return self.message_mlp(tmp)\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        # aggr_out: Aggregated messages [N, out_features]\n",
    "        return self.update_mlp(aggr_out)\n",
    "    \n",
    "def weighted_mse_loss(output, target, score_threshold=0.8, high_score_weight=10):\n",
    "    # Assumes output and target are the predicted and true scores, respectively\n",
    "    weights = torch.ones_like(target)\n",
    "    high_score_mask = target >= score_threshold\n",
    "    weights[high_score_mask] = high_score_weight\n",
    "    return ((output - target) ** 2 * weights).mean()\n",
    "\n",
    "def generate_single_graph(sequencing, scores):\n",
    "    # Load and preprocess sequencing data\n",
    "    gene_df = pd.read_csv(sequencing)\n",
    "    gene_df = gene_df.rename(columns={\"prc\": \"well_id\", \"grna\": \"gene_id\", \"count\": \"read_count\"})\n",
    "    total_reads_per_well = gene_df.groupby('well_id')['read_count'].sum().reset_index(name='total_reads')\n",
    "    gene_df = gene_df.merge(total_reads_per_well, on='well_id')\n",
    "    gene_df['well_read_fraction'] = gene_df['read_count']/gene_df['total_reads']\n",
    "\n",
    "    # Load and preprocess cell score data\n",
    "    cell_df = pd.read_csv(scores)\n",
    "    cell_df = cell_df[['prcfo', 'prc', 'pred']].rename(columns={'prcfo': 'cell_id', 'prc': 'well_id', 'pred': 'score'})\n",
    "\n",
    "    # Initialize mappings\n",
    "    gene_id_to_index = {gene: i for i, gene in enumerate(gene_df['gene_id'].unique())}\n",
    "    cell_id_to_index = {cell: i + len(gene_id_to_index) for i, cell in enumerate(cell_df['cell_id'].unique())}\n",
    "\n",
    "    # Initialize edge indices and attributes\n",
    "    edge_index = []\n",
    "    edge_attr = []\n",
    "\n",
    "    # Associate each cell with all genes in the same well\n",
    "    for well_id, group in gene_df.groupby('well_id'):\n",
    "        if well_id in cell_df['well_id'].values:\n",
    "            cell_indices = cell_df[cell_df['well_id'] == well_id]['cell_id'].map(cell_id_to_index).values\n",
    "            gene_indices = group['gene_id'].map(gene_id_to_index).values\n",
    "            fractions = group['well_read_fraction'].values\n",
    "            \n",
    "            for cell_idx in cell_indices:\n",
    "                for gene_idx, fraction in zip(gene_indices, fractions):\n",
    "                    edge_index.append([cell_idx, gene_idx])\n",
    "                    edge_attr.append([fraction])\n",
    "\n",
    "    # Convert lists to PyTorch tensors\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "    cell_scores = torch.tensor(cell_df['score'].values, dtype=torch.float)\n",
    "\n",
    "    # One-hot encoding for genes, and zero features for cells (could be replaced with real features if available)\n",
    "    gene_features = torch.eye(len(gene_id_to_index))\n",
    "    cell_features = torch.zeros(len(cell_id_to_index), gene_features.size(1))\n",
    "\n",
    "    # Combine features\n",
    "    x = torch.cat([cell_features, gene_features], dim=0)\n",
    "\n",
    "    # Create the graph data object\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=cell_scores)\n",
    "\n",
    "    return data, gene_id_to_index, len(gene_id_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ba856250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch_geometric.data import Data\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from torch.nn import Sequential, Linear, ReLU\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import degree, add_self_loops, softmax\n",
    "from torch_geometric.loader import DataLoader, NeighborSampler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch_geometric.nn import SAGEConv, global_mean_pool, Linear, TransformerConv, GCNConv, GATConv, MessagePassing\n",
    "from torch import Tensor, nn\n",
    "from torch_geometric.data import Batch\n",
    "from torch.utils.data import DataLoader as TorchDataLoader\n",
    "from torch.nn import Linear, Module\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, Module\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn.inits import reset\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "\n",
    "def collate(batch):\n",
    "    data_list = [data for _, data in batch]\n",
    "    return Batch.from_data_list(data_list)\n",
    "\n",
    "\n",
    "def generate_well_graphs(sequencing, scores):\n",
    "    # Load and preprocess sequencing data\n",
    "    gene_df = pd.read_csv(sequencing)\n",
    "    gene_df = gene_df.rename(columns={'prc': 'well_id', 'grna': 'gene_id', 'count': 'read_count'})\n",
    "    total_reads_per_well = gene_df.groupby('well_id')['read_count'].sum().reset_index(name='total_reads')\n",
    "    gene_df = gene_df.merge(total_reads_per_well, on='well_id')\n",
    "    gene_df['well_read_fraction'] = gene_df['read_count'] / gene_df['total_reads']\n",
    "\n",
    "    # Load and preprocess cell score data\n",
    "    cell_df = pd.read_csv(scores)\n",
    "    cell_df = cell_df[['prcfo', 'prc', 'pred']].rename(columns={'prcfo': 'cell_id', 'prc': 'well_id', 'pred': 'score'})\n",
    "\n",
    "    # Initialize mappings\n",
    "    gene_id_to_index = {gene: i for i, gene in enumerate(gene_df['gene_id'].unique())}\n",
    "    cell_id_to_index = {cell: i + len(gene_id_to_index) for i, cell in enumerate(cell_df['cell_id'].unique())}\n",
    "\n",
    "    # Initialize a dictionary to store edge information for each well subgraph\n",
    "    wells_subgraphs = defaultdict(lambda: {'edge_index': [], 'edge_attr': []})\n",
    "\n",
    "    # Associate each cell with all genes in the same well\n",
    "    for well_id, group in gene_df.groupby('well_id'):\n",
    "        if well_id in cell_df['well_id'].values:\n",
    "            cell_indices = cell_df[cell_df['well_id'] == well_id]['cell_id'].map(cell_id_to_index).values\n",
    "            gene_indices = group['gene_id'].map(gene_id_to_index).values\n",
    "            fractions = group['well_read_fraction'].values\n",
    "\n",
    "            for cell_idx in cell_indices:\n",
    "                for gene_idx, fraction in zip(gene_indices, fractions):\n",
    "                    wells_subgraphs[well_id]['edge_index'].append([cell_idx, gene_idx])\n",
    "                    wells_subgraphs[well_id]['edge_attr'].append([fraction])\n",
    "\n",
    "    # Process well subgraphs into PyTorch Geometric Data objects\n",
    "    well_data_list = []\n",
    "    for well_id, subgraph in wells_subgraphs.items():\n",
    "        edge_index = torch.tensor(subgraph['edge_index'], dtype=torch.long).t().contiguous()\n",
    "        edge_attr = torch.tensor(subgraph['edge_attr'], dtype=torch.float)\n",
    "        num_nodes = max(max(edge) for edge in subgraph['edge_index']) + 1\n",
    "        x = torch.ones((num_nodes, 1))  # Feature matrix with a single feature set to 1 for each node\n",
    "\n",
    "        # Retrieve cell scores for the current well\n",
    "        cell_scores = cell_df[cell_df['well_id'] == well_id]['score'].values\n",
    "        # Create a tensor for cell scores, ensuring the order matches that of the nodes in the graph\n",
    "        y = torch.tensor(cell_scores, dtype=torch.float)\n",
    "        \n",
    "        subgraph_data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "        well_data_list.append((well_id, subgraph_data))\n",
    "    \n",
    "    return well_data_list, gene_id_to_index, len(gene_id_to_index), cell_id_to_index\n",
    "\n",
    "class CustomTransformerConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, heads=1, concat=True, beta=False, dropout=0.0, edge_dim=None):\n",
    "        super().__init__(node_dim=0, aggr='add')  # Specify aggregation\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.concat = concat\n",
    "        self.beta = beta\n",
    "        self.dropout = dropout\n",
    "        self.edge_dim = edge_dim\n",
    "\n",
    "        # The linear layers for the multi-head attention mechanism\n",
    "        self.lin_query = Linear(in_channels, heads * out_channels, bias=False)\n",
    "        self.lin_key = Linear(in_channels, heads * out_channels, bias=False)\n",
    "        self.lin_value = Linear(in_channels, heads * out_channels, bias=False)\n",
    "\n",
    "        # Optional edge transformation\n",
    "        if edge_dim is not None:\n",
    "            self.lin_edge = Linear(edge_dim, heads * out_channels, bias=False)\n",
    "\n",
    "        # Optional beta parameter for combining aggregation and skip connection\n",
    "        if self.beta:\n",
    "            self.lin_gate = torch.nn.Linear(in_channels + out_channels, 1, bias=True)\n",
    "        \n",
    "        # The final linear transformation that is applied to each node feature vector\n",
    "        self.lin_out = Linear(heads * out_channels, out_channels, bias=True) if concat else Linear(out_channels, out_channels, bias=True)\n",
    "\n",
    "        # For storing the attention weights\n",
    "        self.att = None \n",
    "\n",
    "        # Initialize the parameters\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Reset the parameters here\n",
    "        self.lin_query.reset_parameters()\n",
    "        self.lin_key.reset_parameters()\n",
    "        self.lin_value.reset_parameters()\n",
    "        if self.edge_dim is not None:\n",
    "            self.lin_edge.reset_parameters()\n",
    "        if self.beta:\n",
    "            self.lin_gate.reset_parameters()\n",
    "        self.lin_out.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "        print(f\"Input features shape: {x.shape}\")\n",
    "        query = self.lin_query(x).view(-1, self.heads, self.out_channels)\n",
    "        query = self.lin_query(x)\n",
    "        print(f\"Query shape (pre-view): {query.shape}\")\n",
    "        key = self.lin_key(x).view(-1, self.heads, self.out_channels)\n",
    "        value = self.lin_value(x).view(-1, self.heads, self.out_channels)\n",
    "        \n",
    "        # Propagate the messages\n",
    "        out = self.propagate(edge_index, x=(query, key, value), edge_attr=edge_attr, size=None)\n",
    "        \n",
    "        # Reshape and concatenate head outputs if required\n",
    "        if self.concat:\n",
    "            out = out.view(-1, self.heads * self.out_channels)\n",
    "        else:\n",
    "            out = out.mean(dim=1)\n",
    "        \n",
    "        # Apply root node transformation with skip connection if required\n",
    "        if self.root_weight:\n",
    "            out = out + self.lin_root(x[:out.size(0), :])\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def message(self, x_j, x_i, edge_attr, index, ptr, size_i):\n",
    "        # Compute messages\n",
    "        # This needs to be implemented based on your model's specifics\n",
    "        query, key, value = x_i[0], x_j[1], x_j[2]\n",
    "        # Compute the attention scores\n",
    "        alpha = (query * key).sum(dim=-1) / self.scale\n",
    "        alpha = softmax(alpha, index, ptr, size_i)\n",
    "        \n",
    "        # Apply attention scores to the values\n",
    "        out = value * alpha.view(-1, self.heads, 1)\n",
    "        return out.view(-1, self.heads * self.out_channels)\n",
    "\n",
    "\n",
    "class GraphTransformer(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, dropout_rate=0.1):\n",
    "        super(GraphTransformer, self).__init__()\n",
    "        # Assuming you want to predict a single value per graph, adjust the out_channels as needed.\n",
    "        num_heads = 4  # Example: 4 attention heads\n",
    "        out_channels = 1  # Example: predicting a single score per graph\n",
    "        self.conv1 = CustomTransformerConv(num_node_features, 128, heads=num_heads, dropout=dropout_rate, edge_dim=1)\n",
    "        self.conv2 = CustomTransformerConv(128 * num_heads, 256, heads=num_heads, dropout=dropout_rate, edge_dim=1)\n",
    "        self.lin = Linear(256 * num_heads, out_channels)  # Adjusted for a single output feature\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "\n",
    "        # Here we call forward on the CustomTransformerConv.\n",
    "        # Make sure edge_attr is only passed if you have edge features.\n",
    "        # Adjust the head dimensions and any additional logic based on your architecture.\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_attr=edge_attr))\n",
    "        # more layers...\n",
    "\n",
    "        return x\n",
    "\n",
    "def train_graph_network(graph_data_list, feature_size, model_path, batch_size=8, epochs=100, lr=0.001):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = GraphTransformer(num_node_features=feature_size).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "    data_loader = TorchDataLoader(graph_data_list, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for data in data_loader:\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data)\n",
    "            loss = criterion(out.view(-1), data.y.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(data_loader)}')\n",
    "    \n",
    "    torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4cec73e7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input features shape: torch.Size([2141614, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2141614x1 and 1132x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [39]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_graph_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph_data_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfeature_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/olafsson/Desktop/gnn/model/pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [38]\u001b[0m, in \u001b[0;36mtrain_graph_network\u001b[0;34m(graph_data_list, feature_size, model_path, batch_size, epochs, lr)\u001b[0m\n\u001b[1;32m    194\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    195\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 196\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), data\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    198\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/spacr_package/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/spacr_package/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Input \u001b[0;32mIn [38]\u001b[0m, in \u001b[0;36mGraphTransformer.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    170\u001b[0m x, edge_index, edge_attr, batch \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index, data\u001b[38;5;241m.\u001b[39medge_attr, data\u001b[38;5;241m.\u001b[39mbatch\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# Here we call forward on the CustomTransformerConv.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# Make sure edge_attr is only passed if you have edge features.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# Adjust the head dimensions and any additional logic based on your architecture.\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# more layers...\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/spacr_package/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/spacr_package/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Input \u001b[0;32mIn [38]\u001b[0m, in \u001b[0;36mCustomTransformerConv.forward\u001b[0;34m(self, x, edge_index, edge_attr)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge_index, edge_attr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput features shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 125\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlin_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_channels)\n\u001b[1;32m    126\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin_query(x)\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery shape (pre-view): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/spacr_package/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/spacr_package/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/spacr_package/lib/python3.9/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2141614x1 and 1132x512)"
     ]
    }
   ],
   "source": [
    "train_graph_network(graph_data_list=graph_data,\n",
    "                    feature_size=feature_size,\n",
    "                    model_path='/home/olafsson/Desktop/gnn/model/pth',\n",
    "                    batch_size=8,\n",
    "                    epochs=100,\n",
    "                    lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e98de63",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequencing = '/mnt/data/CellVoyager/20x/tsg101/crispr_screen/all/measurements/sequencing.csv'\n",
    "scores = '/mnt/data/CellVoyager/20x/tsg101/crispr_screen/all/measurements/dv_cell.csv'\n",
    "graph_data, gene_id_to_index, feature_size, cell_id_to_index = generate_well_graphs(sequencing,scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c90897d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ba4227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626abe26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060ecbaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a888cfa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059289e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032873cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1e2f83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ca3542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324569d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948fe4ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d6a9cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c422ad17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbb7555",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472a6050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a93098e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b3f5be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7a8eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f3769e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a169e91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f857827",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99979935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0c4387",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890a222a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f483a27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7876f8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe02a9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8687cb6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_save_path = '/home/olafsson/Desktop/gnn/mode.pth'\n",
    "compute_gene_importance(model,\n",
    "                        graph_data,\n",
    "                        model_save_path,\n",
    "                        n_permutations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2c6688",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequencing = '/mnt/data/CellVoyager/20x/tsg101/crispr_screen/all/measurements/sequencing.csv'\n",
    "score = '/mnt/data/CellVoyager/20x/tsg101/crispr_screen/all/measurements/dv_cell.csv'\n",
    "\n",
    "# Example loading step\n",
    "gene_df = pd.read_csv(sequencing)\n",
    "cell_df = pd.read_csv(score)\n",
    "\n",
    "gene_df = gene_df.rename(columns={\"prc\": \"well_id\", \"grna\": \"gene_id\", \"count\": \"read_count\"})\n",
    "gene_df = gene_df.drop(columns=['Unnamed: 0', 'plate', 'row', 'col', 'grna_seq', 'gene'])\n",
    "total_reads_per_well = gene_df.groupby('well_id')['read_count'].sum().reset_index(name='total_reads')\n",
    "gene_df = gene_df.merge(total_reads_per_well, on='well_id')\n",
    "gene_df['well_read_fraction'] = gene_df['read_count']/gene_df['total_reads']\n",
    "gene_df = gene_df.drop(columns=['read_count', 'total_reads'])\n",
    "\n",
    "cell_df = cell_df.rename(columns={\"prcfo\": \"cell_id\", \"prc\": \"well_id\", \"pred\": \"score\"})\n",
    "cell_df = cell_df.drop(columns=['parasite_area', 'parasite_area', 'recruitment'])\n",
    "display(gene_df)\n",
    "display(cell_df)\n",
    "feature_size = len(gene_df['gene_id'].unique())\n",
    "feature_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f643f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'feature_size: {feature_size}')\n",
    "graph_data, gene_id_to_index = generate_graph(gene_df,cell_df)\n",
    "dict_file_path = '/home/olafsson/Desktop/gnn/dict.pth'\n",
    "\n",
    "with open(dict_file_path, 'wb') as file:\n",
    "    pickle.dump(gene_id_to_index, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1d794d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'pip install torch-sparse -f https://data.pyg.org/whl/torch-2.2.1+cu121.html'\n",
    "'pip install torch-scatter -f https://data.pyg.org/whl/torch-2.2.1+cu121.html'\n",
    "print(f'feature_size: {feature_size}')\n",
    "\n",
    "dict_file_path = '/home/olafsson/Desktop/gnn/dict.pth'\n",
    "\n",
    "with open(dict_file_path, 'wb') as file:\n",
    "    pickle.dump(gene_id_to_index, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477d7156",
   "metadata": {},
   "outputs": [],
   "source": [
    "I first transect a library of gRNAs targeting ~1400 genes into Toxoplasma tachyzoites and grow the parasites under selection for 1 week. This generates a pooled population of mutant parasites, each parasite is missing one gene. I then seed HFF cells in 384 well plates and transfer 10 mutants on average to each well. These parasites grow for a few days to generate sub-pools of parasite populations consisting of on average 10 unique mutants. At this point i transfer mutants to corresponding wells in new 384 well plates, these plates have cells that the parasites will infect. I then fix, stain and image these new plates. The rest of the parasites in the original 384 well plates are sequenced so i know which mutants were present in each well. Single cell images are then cropped from each field of view classified by a CNN. I only include cells infected by one parasite. So at the end of the experiment i have infected cells infected by 1 mutant parasite with phenotype scores and i know the genes that are knocked out in the parasites in each well. I also know the relative abundance of each mutant in each well through the proportion of sequencing reads in each well. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacr_package",
   "language": "python",
   "name": "spacr_package"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
