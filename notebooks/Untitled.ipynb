{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f736bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def generate_graphs(sequencing, scores, cell_min, gene_min_read):\n",
    "    # Load and preprocess sequencing (gene) data\n",
    "    gene_df = pd.read_csv(sequencing)\n",
    "    gene_df = gene_df.rename(columns={'prc': 'well_id', 'grna': 'gene_id', 'count': 'read_count'})\n",
    "    # Filter out genes with read counts less than gene_min_read\n",
    "    gene_df = gene_df[gene_df['read_count'] >= gene_min_read]\n",
    "    total_reads_per_well = gene_df.groupby('well_id')['read_count'].sum().reset_index(name='total_reads')\n",
    "    gene_df = gene_df.merge(total_reads_per_well, on='well_id')\n",
    "    gene_df['well_read_fraction'] = gene_df['read_count'] / gene_df['total_reads']\n",
    "\n",
    "    # Mapping genes to indices\n",
    "    gene_id_to_index = {gene: i for i, gene in enumerate(gene_df['gene_id'].unique())}\n",
    "    feature_size = len(gene_id_to_index)\n",
    "\n",
    "    # Load and preprocess cell score data\n",
    "    cell_df = pd.read_csv(scores)\n",
    "    cell_df = cell_df[['prcfo', 'prc', 'pred']].rename(columns={'prcfo': 'cell_id', 'prc': 'well_id', 'pred': 'score'})\n",
    "\n",
    "    graphs = []\n",
    "    for well_id in pd.unique(gene_df['well_id']):\n",
    "        well_genes = gene_df[gene_df['well_id'] == well_id]\n",
    "        well_cells = cell_df[cell_df['well_id'] == well_id]\n",
    "        \n",
    "        # Skip this well if the number of cells is less than cell_min\n",
    "        if well_cells.empty or well_genes.empty or len(well_cells) < cell_min:\n",
    "            continue\n",
    "        \n",
    "        # Prepare gene features (well_read_fraction)\n",
    "        gene_features = torch.tensor(well_genes['well_read_fraction'].values, dtype=torch.float).view(-1, 1)\n",
    "        # Prepare cell features (scores)\n",
    "        cell_features = torch.tensor(well_cells['score'].values, dtype=torch.float).view(-1, 1)\n",
    "\n",
    "        num_genes = gene_features.size(0)\n",
    "        num_cells = cell_features.size(0)\n",
    "        num_nodes = num_genes + num_cells\n",
    "        \n",
    "        # Create dense adjacency matrix connecting each cell to all genes\n",
    "        adj = torch.zeros((num_nodes, num_nodes), dtype=torch.float)\n",
    "        adj[num_genes:, :num_genes] = 1  # Assuming cells come after genes in node ordering\n",
    "\n",
    "        graph = {\n",
    "            'adjacency_matrix': adj,\n",
    "            'gene_features': gene_features,\n",
    "            'cell_features': cell_features,\n",
    "            'num_cells': num_cells,\n",
    "            'num_genes': num_genes\n",
    "        }\n",
    "        graphs.append(graph)\n",
    "    print(f'Generated dataset with {len(graphs)} graphs, and {len(gene_id_to_index)} genes')\n",
    "    return graphs, feature_size, gene_id_to_index\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, attn_dim, dropout_rate=0.1):\n",
    "        super(Attention, self).__init__()\n",
    "        self.query = nn.Linear(feature_dim, attn_dim)\n",
    "        self.key = nn.Linear(feature_dim, attn_dim)\n",
    "        self.value = nn.Linear(feature_dim, feature_dim)\n",
    "        self.scale = 1.0 / (attn_dim ** 0.5)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, gene_features, cell_features):\n",
    "        # Queries come from the cell features\n",
    "        q = self.query(cell_features)\n",
    "        # Keys and values come from the gene features\n",
    "        k = self.key(gene_features)\n",
    "        v = self.value(gene_features)\n",
    "        \n",
    "        # Compute attention weights\n",
    "        attn_weights = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        # Apply dropout to attention weights\n",
    "        attn_weights = self.dropout(attn_weights)  \n",
    "\n",
    "        # Apply attention weights to the values\n",
    "        attn_output = torch.matmul(attn_weights, v)\n",
    "        \n",
    "        return attn_output, attn_weights\n",
    "\n",
    "class GraphTransformer(nn.Module):\n",
    "    def __init__(self, gene_feature_size, cell_feature_size, hidden_dim, output_dim, attn_dim, dropout_rate=0.1):\n",
    "        super(GraphTransformer, self).__init__()\n",
    "        self.gene_transform = nn.Linear(gene_feature_size, hidden_dim)\n",
    "        self.cell_transform = nn.Linear(cell_feature_size, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Attention layer to let each cell attend to all genes\n",
    "        self.attention = Attention(hidden_dim, attn_dim)\n",
    "\n",
    "        # This layer is used to transform the combined features after attention\n",
    "        self.combine_transform = nn.Linear(2 * hidden_dim, hidden_dim)\n",
    "\n",
    "        # Output layer for predicting cell scores, ensuring it matches the number of cells\n",
    "        self.cell_output = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, adjacency_matrix, gene_features, cell_features):\n",
    "        # Apply initial transformation to gene and cell features\n",
    "        transformed_gene_features = F.relu(self.gene_transform(gene_features))\n",
    "        transformed_cell_features = F.relu(self.cell_transform(cell_features))\n",
    "\n",
    "        # Incorporate attention mechanism\n",
    "        attn_output, attn_weights = self.attention(transformed_gene_features, transformed_cell_features)\n",
    "\n",
    "        # Combine the transformed cell features with the attention output features\n",
    "        combined_cell_features = torch.cat((transformed_cell_features, attn_output), dim=1)\n",
    "        \n",
    "        # Apply dropout here as well\n",
    "        combined_cell_features = self.dropout(combined_cell_features)  \n",
    "\n",
    "        combined_cell_features = F.relu(self.combine_transform(combined_cell_features))\n",
    "\n",
    "        # Combine gene and cell features for message passing\n",
    "        combined_features = torch.cat((transformed_gene_features, combined_cell_features), dim=0)\n",
    "\n",
    "        # Apply message passing via adjacency matrix multiplication\n",
    "        message_passed_features = torch.matmul(adjacency_matrix, combined_features)\n",
    "\n",
    "        # Predict cell scores from the post-message passed cell features\n",
    "        cell_scores = self.cell_output(message_passed_features[-cell_features.size(0):])\n",
    "\n",
    "        return cell_scores, attn_weights\n",
    "    \n",
    "def train_graph_transformer(graphs, lr=0.01, dropout_rate=0.1, epochs=100, save_fldr='', acc_threshold = 0.1):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = GraphTransformer(gene_feature_size=1, cell_feature_size=1, hidden_dim=256, output_dim=1, attn_dim=128, dropout_rate=dropout_rate).to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    training_log = []\n",
    "    \n",
    "    accumulate_grad_batches=1\n",
    "    threshold=acc_threshold\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        optimizer.zero_grad()\n",
    "        batch_count = 0  # Initialize batch_count\n",
    "        \n",
    "        for graph in graphs:\n",
    "            adjacency_matrix = graph['adjacency_matrix'].to(device)\n",
    "            gene_features = graph['gene_features'].to(device)\n",
    "            cell_features = graph['cell_features'].to(device)\n",
    "            num_cells = graph['num_cells']\n",
    "            predictions, attn_weights = model(adjacency_matrix, gene_features, cell_features)\n",
    "            predictions = predictions.squeeze()\n",
    "            true_scores = cell_features[:num_cells, 0]\n",
    "            loss = criterion(predictions, true_scores) / accumulate_grad_batches\n",
    "            loss.backward()\n",
    "\n",
    "            # Calculate \"accuracy\"\n",
    "            with torch.no_grad():\n",
    "                correct_predictions = (torch.abs(predictions - true_scores) / true_scores <= threshold).sum().item()\n",
    "                total_correct += correct_predictions\n",
    "                total_samples += num_cells\n",
    "\n",
    "            batch_count += 1  # Increment batch_count\n",
    "            if batch_count % accumulate_grad_batches == 0 or batch_count == len(graphs):\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item() * accumulate_grad_batches\n",
    "        \n",
    "        accuracy = total_correct / total_samples\n",
    "        training_log.append({\"Epoch\": epoch+1, \"Average Loss\": total_loss / len(graphs), \"Accuracy\": accuracy})\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(graphs)}, Accuracy: {accuracy}\", end=\"\\r\", flush=True)\n",
    "    \n",
    "    # Save the training log and model as before\n",
    "    os.makedirs(save_fldr, exist_ok=True)\n",
    "    log_path = os.path.join(save_fldr, 'training_log.csv')\n",
    "    training_log_df = pd.DataFrame(training_log)\n",
    "    training_log_df.to_csv(log_path, index=False)\n",
    "    print(f\"Training log saved to {log_path}\")\n",
    "    \n",
    "    model_path = os.path.join(save_fldr, 'model.pth')\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "\n",
    "    return model\n",
    "        \n",
    "def annotate_cells_with_genes(graphs, model, gene_id_to_index):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    annotated_data = []\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for graph in graphs:\n",
    "            adjacency_matrix = graph['adjacency_matrix'].to(device)\n",
    "            gene_features = graph['gene_features'].to(device)\n",
    "            cell_features = graph['cell_features'].to(device)\n",
    "\n",
    "            predictions, attn_weights = model(adjacency_matrix, gene_features, cell_features)\n",
    "            predictions = np.atleast_1d(predictions.squeeze().cpu().numpy())\n",
    "            attn_weights = np.atleast_2d(attn_weights.squeeze().cpu().numpy())\n",
    "\n",
    "            if attn_weights.shape[0] != cell_features.size(0):\n",
    "                # Skip if the first dimension of attn_weights does not match the number of cells\n",
    "                continue\n",
    "            \n",
    "            for cell_idx in range(cell_features.size(0)):\n",
    "                true_score = cell_features[cell_idx, 0].item()\n",
    "                predicted_score = predictions[cell_idx]\n",
    "                most_probable_gene_idx = attn_weights[cell_idx].argmax()\n",
    "                most_probable_gene_score = attn_weights[cell_idx, most_probable_gene_idx]\n",
    "\n",
    "                gene_id = list(gene_id_to_index.keys())[most_probable_gene_idx]\n",
    "\n",
    "                annotated_data.append({\n",
    "                    \"Cell ID\": cell_idx,\n",
    "                    \"Most Probable Gene\": gene_id,\n",
    "                    \"Cell Score\": true_score,\n",
    "                    \"Predicted Cell Score\": predicted_score,\n",
    "                    \"Probability Score for Highest Gene\": most_probable_gene_score\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(annotated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb18aaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated dataset with 1860 graphs, and 1054 genes\n"
     ]
    }
   ],
   "source": [
    "sequencing = '/mnt/data/CellVoyager/20x/tsg101/crispr_screen/all/measurements/sequencing.csv'\n",
    "scores = '/mnt/data/CellVoyager/20x/tsg101/crispr_screen/all/measurements/dv_cell.csv'\n",
    "\n",
    "graphs, feature_size, gene_id_to_index = generate_graphs(sequencing, scores, cell_min=50, gene_min_read=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e84785",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41, Loss: 0.15246710189034843, Accuracy: 0.35204392922513734\r"
     ]
    }
   ],
   "source": [
    "model = train_graph_transformer(graphs,\n",
    "                        lr=0.00001,\n",
    "                        dropout_rate=0.1,\n",
    "                        epochs=10000,\n",
    "                        save_fldr='/home/olafsson/Desktop/gnn',\n",
    "                        acc_threshold = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96fd04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = annotate_cells_with_genes(graphs, model, gene_id_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98220ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a996a393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ac0ee1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e808895",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2976eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabd910f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8557010",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125080ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12372bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8133f45a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ea9804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd80c786",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPNN(MessagePassing):\n",
    "    def __init__(self, node_in_features, edge_in_features, out_features):\n",
    "        super(MPNN, self).__init__(aggr='mean')  # 'mean' aggregation.\n",
    "        self.message_mlp = Sequential(\n",
    "            Linear(node_in_features + edge_in_features, 128),\n",
    "            ReLU(),\n",
    "            Linear(128, out_features)\n",
    "        )\n",
    "        self.update_mlp = Sequential(\n",
    "            Linear(out_features, out_features),\n",
    "            ReLU(),\n",
    "            Linear(out_features, out_features)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # x: Node features [N, node_in_features]\n",
    "        # edge_index: Graph connectivity [2, E]\n",
    "        # edge_attr: Edge attributes/features [E, edge_in_features]\n",
    "        return self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "\n",
    "    def message(self, x_j, edge_attr):\n",
    "        # x_j: Input features of neighbors [E, node_in_features]\n",
    "        # edge_attr: Edge attributes [E, edge_in_features]\n",
    "        tmp = torch.cat([x_j, edge_attr], dim=-1)  # Concatenate node features with edge attributes\n",
    "        return self.message_mlp(tmp)\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        # aggr_out: Aggregated messages [N, out_features]\n",
    "        return self.update_mlp(aggr_out)\n",
    "    \n",
    "def weighted_mse_loss(output, target, score_threshold=0.8, high_score_weight=10):\n",
    "    # Assumes output and target are the predicted and true scores, respectively\n",
    "    weights = torch.ones_like(target)\n",
    "    high_score_mask = target >= score_threshold\n",
    "    weights[high_score_mask] = high_score_weight\n",
    "    return ((output - target) ** 2 * weights).mean()\n",
    "\n",
    "def generate_single_graph(sequencing, scores):\n",
    "    # Load and preprocess sequencing data\n",
    "    gene_df = pd.read_csv(sequencing)\n",
    "    gene_df = gene_df.rename(columns={\"prc\": \"well_id\", \"grna\": \"gene_id\", \"count\": \"read_count\"})\n",
    "    total_reads_per_well = gene_df.groupby('well_id')['read_count'].sum().reset_index(name='total_reads')\n",
    "    gene_df = gene_df.merge(total_reads_per_well, on='well_id')\n",
    "    gene_df['well_read_fraction'] = gene_df['read_count']/gene_df['total_reads']\n",
    "\n",
    "    # Load and preprocess cell score data\n",
    "    cell_df = pd.read_csv(scores)\n",
    "    cell_df = cell_df[['prcfo', 'prc', 'pred']].rename(columns={'prcfo': 'cell_id', 'prc': 'well_id', 'pred': 'score'})\n",
    "\n",
    "    # Initialize mappings\n",
    "    gene_id_to_index = {gene: i for i, gene in enumerate(gene_df['gene_id'].unique())}\n",
    "    cell_id_to_index = {cell: i + len(gene_id_to_index) for i, cell in enumerate(cell_df['cell_id'].unique())}\n",
    "\n",
    "    # Initialize edge indices and attributes\n",
    "    edge_index = []\n",
    "    edge_attr = []\n",
    "\n",
    "    # Associate each cell with all genes in the same well\n",
    "    for well_id, group in gene_df.groupby('well_id'):\n",
    "        if well_id in cell_df['well_id'].values:\n",
    "            cell_indices = cell_df[cell_df['well_id'] == well_id]['cell_id'].map(cell_id_to_index).values\n",
    "            gene_indices = group['gene_id'].map(gene_id_to_index).values\n",
    "            fractions = group['well_read_fraction'].values\n",
    "            \n",
    "            for cell_idx in cell_indices:\n",
    "                for gene_idx, fraction in zip(gene_indices, fractions):\n",
    "                    edge_index.append([cell_idx, gene_idx])\n",
    "                    edge_attr.append([fraction])\n",
    "\n",
    "    # Convert lists to PyTorch tensors\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "    cell_scores = torch.tensor(cell_df['score'].values, dtype=torch.float)\n",
    "\n",
    "    # One-hot encoding for genes, and zero features for cells (could be replaced with real features if available)\n",
    "    gene_features = torch.eye(len(gene_id_to_index))\n",
    "    cell_features = torch.zeros(len(cell_id_to_index), gene_features.size(1))\n",
    "\n",
    "    # Combine features\n",
    "    x = torch.cat([cell_features, gene_features], dim=0)\n",
    "\n",
    "    # Create the graph data object\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=cell_scores)\n",
    "\n",
    "    return data, gene_id_to_index, len(gene_id_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefc8722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch_geometric.data import Data\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from torch.nn import Sequential, Linear, ReLU\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import degree, add_self_loops, softmax\n",
    "from torch_geometric.loader import DataLoader, NeighborSampler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch_geometric.nn import SAGEConv, global_mean_pool, Linear, TransformerConv, GCNConv, GATConv, MessagePassing\n",
    "from torch import Tensor, nn\n",
    "from torch_geometric.data import Batch\n",
    "from torch.utils.data import DataLoader as TorchDataLoader\n",
    "from torch.nn import Linear, Module\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, Module\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn.inits import reset\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "\n",
    "def collate(batch):\n",
    "    data_list = [data for _, data in batch]\n",
    "    return Batch.from_data_list(data_list)\n",
    "\n",
    "\n",
    "def generate_well_graphs(sequencing, scores):\n",
    "    # Load and preprocess sequencing data\n",
    "    gene_df = pd.read_csv(sequencing)\n",
    "    gene_df = gene_df.rename(columns={'prc': 'well_id', 'grna': 'gene_id', 'count': 'read_count'})\n",
    "    total_reads_per_well = gene_df.groupby('well_id')['read_count'].sum().reset_index(name='total_reads')\n",
    "    gene_df = gene_df.merge(total_reads_per_well, on='well_id')\n",
    "    gene_df['well_read_fraction'] = gene_df['read_count'] / gene_df['total_reads']\n",
    "\n",
    "    # Load and preprocess cell score data\n",
    "    cell_df = pd.read_csv(scores)\n",
    "    cell_df = cell_df[['prcfo', 'prc', 'pred']].rename(columns={'prcfo': 'cell_id', 'prc': 'well_id', 'pred': 'score'})\n",
    "\n",
    "    # Initialize mappings\n",
    "    gene_id_to_index = {gene: i for i, gene in enumerate(gene_df['gene_id'].unique())}\n",
    "    cell_id_to_index = {cell: i + len(gene_id_to_index) for i, cell in enumerate(cell_df['cell_id'].unique())}\n",
    "\n",
    "    # Initialize a dictionary to store edge information for each well subgraph\n",
    "    wells_subgraphs = defaultdict(lambda: {'edge_index': [], 'edge_attr': []})\n",
    "\n",
    "    # Associate each cell with all genes in the same well\n",
    "    for well_id, group in gene_df.groupby('well_id'):\n",
    "        if well_id in cell_df['well_id'].values:\n",
    "            cell_indices = cell_df[cell_df['well_id'] == well_id]['cell_id'].map(cell_id_to_index).values\n",
    "            gene_indices = group['gene_id'].map(gene_id_to_index).values\n",
    "            fractions = group['well_read_fraction'].values\n",
    "\n",
    "            for cell_idx in cell_indices:\n",
    "                for gene_idx, fraction in zip(gene_indices, fractions):\n",
    "                    wells_subgraphs[well_id]['edge_index'].append([cell_idx, gene_idx])\n",
    "                    wells_subgraphs[well_id]['edge_attr'].append([fraction])\n",
    "\n",
    "    # Process well subgraphs into PyTorch Geometric Data objects\n",
    "    well_data_list = []\n",
    "    for well_id, subgraph in wells_subgraphs.items():\n",
    "        edge_index = torch.tensor(subgraph['edge_index'], dtype=torch.long).t().contiguous()\n",
    "        edge_attr = torch.tensor(subgraph['edge_attr'], dtype=torch.float)\n",
    "        num_nodes = max(max(edge) for edge in subgraph['edge_index']) + 1\n",
    "        x = torch.ones((num_nodes, 1))  # Feature matrix with a single feature set to 1 for each node\n",
    "\n",
    "        # Retrieve cell scores for the current well\n",
    "        cell_scores = cell_df[cell_df['well_id'] == well_id]['score'].values\n",
    "        # Create a tensor for cell scores, ensuring the order matches that of the nodes in the graph\n",
    "        y = torch.tensor(cell_scores, dtype=torch.float)\n",
    "        \n",
    "        subgraph_data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "        well_data_list.append((well_id, subgraph_data))\n",
    "    \n",
    "    return well_data_list, gene_id_to_index, len(gene_id_to_index), cell_id_to_index\n",
    "\n",
    "class CustomTransformerConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, heads=1, concat=True, beta=False, dropout=0.0, edge_dim=None):\n",
    "        super().__init__(node_dim=0, aggr='add')  # Specify aggregation\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.concat = concat\n",
    "        self.beta = beta\n",
    "        self.dropout = dropout\n",
    "        self.edge_dim = edge_dim\n",
    "\n",
    "        # The linear layers for the multi-head attention mechanism\n",
    "        self.lin_query = Linear(in_channels, heads * out_channels, bias=False)\n",
    "        self.lin_key = Linear(in_channels, heads * out_channels, bias=False)\n",
    "        self.lin_value = Linear(in_channels, heads * out_channels, bias=False)\n",
    "\n",
    "        # Optional edge transformation\n",
    "        if edge_dim is not None:\n",
    "            self.lin_edge = Linear(edge_dim, heads * out_channels, bias=False)\n",
    "\n",
    "        # Optional beta parameter for combining aggregation and skip connection\n",
    "        if self.beta:\n",
    "            self.lin_gate = torch.nn.Linear(in_channels + out_channels, 1, bias=True)\n",
    "        \n",
    "        # The final linear transformation that is applied to each node feature vector\n",
    "        self.lin_out = Linear(heads * out_channels, out_channels, bias=True) if concat else Linear(out_channels, out_channels, bias=True)\n",
    "\n",
    "        # For storing the attention weights\n",
    "        self.att = None \n",
    "\n",
    "        # Initialize the parameters\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Reset the parameters here\n",
    "        self.lin_query.reset_parameters()\n",
    "        self.lin_key.reset_parameters()\n",
    "        self.lin_value.reset_parameters()\n",
    "        if self.edge_dim is not None:\n",
    "            self.lin_edge.reset_parameters()\n",
    "        if self.beta:\n",
    "            self.lin_gate.reset_parameters()\n",
    "        self.lin_out.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "        print(f\"Input features shape: {x.shape}\")\n",
    "        query = self.lin_query(x).view(-1, self.heads, self.out_channels)\n",
    "        query = self.lin_query(x)\n",
    "        print(f\"Query shape (pre-view): {query.shape}\")\n",
    "        key = self.lin_key(x).view(-1, self.heads, self.out_channels)\n",
    "        value = self.lin_value(x).view(-1, self.heads, self.out_channels)\n",
    "        \n",
    "        # Propagate the messages\n",
    "        out = self.propagate(edge_index, x=(query, key, value), edge_attr=edge_attr, size=None)\n",
    "        \n",
    "        # Reshape and concatenate head outputs if required\n",
    "        if self.concat:\n",
    "            out = out.view(-1, self.heads * self.out_channels)\n",
    "        else:\n",
    "            out = out.mean(dim=1)\n",
    "        \n",
    "        # Apply root node transformation with skip connection if required\n",
    "        if self.root_weight:\n",
    "            out = out + self.lin_root(x[:out.size(0), :])\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def message(self, x_j, x_i, edge_attr, index, ptr, size_i):\n",
    "        # Compute messages\n",
    "        # This needs to be implemented based on your model's specifics\n",
    "        query, key, value = x_i[0], x_j[1], x_j[2]\n",
    "        # Compute the attention scores\n",
    "        alpha = (query * key).sum(dim=-1) / self.scale\n",
    "        alpha = softmax(alpha, index, ptr, size_i)\n",
    "        \n",
    "        # Apply attention scores to the values\n",
    "        out = value * alpha.view(-1, self.heads, 1)\n",
    "        return out.view(-1, self.heads * self.out_channels)\n",
    "\n",
    "\n",
    "class GraphTransformer(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, dropout_rate=0.1):\n",
    "        super(GraphTransformer, self).__init__()\n",
    "        # Assuming you want to predict a single value per graph, adjust the out_channels as needed.\n",
    "        num_heads = 4  # Example: 4 attention heads\n",
    "        out_channels = 1  # Example: predicting a single score per graph\n",
    "        self.conv1 = CustomTransformerConv(num_node_features, 128, heads=num_heads, dropout=dropout_rate, edge_dim=1)\n",
    "        self.conv2 = CustomTransformerConv(128 * num_heads, 256, heads=num_heads, dropout=dropout_rate, edge_dim=1)\n",
    "        self.lin = Linear(256 * num_heads, out_channels)  # Adjusted for a single output feature\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "\n",
    "        # Here we call forward on the CustomTransformerConv.\n",
    "        # Make sure edge_attr is only passed if you have edge features.\n",
    "        # Adjust the head dimensions and any additional logic based on your architecture.\n",
    "\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_attr=edge_attr))\n",
    "        # more layers...\n",
    "\n",
    "        return x\n",
    "\n",
    "def train_graph_network(graph_data_list, feature_size, model_path, batch_size=8, epochs=100, lr=0.001):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = GraphTransformer(num_node_features=feature_size).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "    data_loader = TorchDataLoader(graph_data_list, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for data in data_loader:\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data)\n",
    "            loss = criterion(out.view(-1), data.y.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(data_loader)}')\n",
    "    \n",
    "    torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4f856a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_graph_network(graph_data_list=graph_data,\n",
    "                    feature_size=feature_size,\n",
    "                    model_path='/home/olafsson/Desktop/gnn/model/pth',\n",
    "                    batch_size=8,\n",
    "                    epochs=100,\n",
    "                    lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bbd3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequencing = '/mnt/data/CellVoyager/20x/tsg101/crispr_screen/all/measurements/sequencing.csv'\n",
    "scores = '/mnt/data/CellVoyager/20x/tsg101/crispr_screen/all/measurements/dv_cell.csv'\n",
    "graph_data, gene_id_to_index, feature_size, cell_id_to_index = generate_well_graphs(sequencing,scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c751fb74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb67bf92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3ad4bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5aecb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712d988c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb49b49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f97179",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e099cbd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf11428",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8d3844",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da27ed4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14906c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939fb07d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974d2c47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e73a45f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd27bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e2b9d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4773de9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194e5e10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea345e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac19159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a90b26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6039ded",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f67f54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e85eb2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8520b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beec08a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6467b68",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_save_path = '/home/olafsson/Desktop/gnn/mode.pth'\n",
    "compute_gene_importance(model,\n",
    "                        graph_data,\n",
    "                        model_save_path,\n",
    "                        n_permutations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49af3afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequencing = '/mnt/data/CellVoyager/20x/tsg101/crispr_screen/all/measurements/sequencing.csv'\n",
    "score = '/mnt/data/CellVoyager/20x/tsg101/crispr_screen/all/measurements/dv_cell.csv'\n",
    "\n",
    "# Example loading step\n",
    "gene_df = pd.read_csv(sequencing)\n",
    "cell_df = pd.read_csv(score)\n",
    "\n",
    "gene_df = gene_df.rename(columns={\"prc\": \"well_id\", \"grna\": \"gene_id\", \"count\": \"read_count\"})\n",
    "gene_df = gene_df.drop(columns=['Unnamed: 0', 'plate', 'row', 'col', 'grna_seq', 'gene'])\n",
    "total_reads_per_well = gene_df.groupby('well_id')['read_count'].sum().reset_index(name='total_reads')\n",
    "gene_df = gene_df.merge(total_reads_per_well, on='well_id')\n",
    "gene_df['well_read_fraction'] = gene_df['read_count']/gene_df['total_reads']\n",
    "gene_df = gene_df.drop(columns=['read_count', 'total_reads'])\n",
    "\n",
    "cell_df = cell_df.rename(columns={\"prcfo\": \"cell_id\", \"prc\": \"well_id\", \"pred\": \"score\"})\n",
    "cell_df = cell_df.drop(columns=['parasite_area', 'parasite_area', 'recruitment'])\n",
    "display(gene_df)\n",
    "display(cell_df)\n",
    "feature_size = len(gene_df['gene_id'].unique())\n",
    "feature_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95e0da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'feature_size: {feature_size}')\n",
    "graph_data, gene_id_to_index = generate_graph(gene_df,cell_df)\n",
    "dict_file_path = '/home/olafsson/Desktop/gnn/dict.pth'\n",
    "\n",
    "with open(dict_file_path, 'wb') as file:\n",
    "    pickle.dump(gene_id_to_index, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a9b17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'pip install torch-sparse -f https://data.pyg.org/whl/torch-2.2.1+cu121.html'\n",
    "'pip install torch-scatter -f https://data.pyg.org/whl/torch-2.2.1+cu121.html'\n",
    "print(f'feature_size: {feature_size}')\n",
    "\n",
    "dict_file_path = '/home/olafsson/Desktop/gnn/dict.pth'\n",
    "\n",
    "with open(dict_file_path, 'wb') as file:\n",
    "    pickle.dump(gene_id_to_index, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab4ebf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "I first transect a library of gRNAs targeting ~1400 genes into Toxoplasma tachyzoites and grow the parasites under selection for 1 week. This generates a pooled population of mutant parasites, each parasite is missing one gene. I then seed HFF cells in 384 well plates and transfer 10 mutants on average to each well. These parasites grow for a few days to generate sub-pools of parasite populations consisting of on average 10 unique mutants. At this point i transfer mutants to corresponding wells in new 384 well plates, these plates have cells that the parasites will infect. I then fix, stain and image these new plates. The rest of the parasites in the original 384 well plates are sequenced so i know which mutants were present in each well. Single cell images are then cropped from each field of view classified by a CNN. I only include cells infected by one parasite. So at the end of the experiment i have infected cells infected by 1 mutant parasite with phenotype scores and i know the genes that are knocked out in the parasites in each well. I also know the relative abundance of each mutant in each well through the proportion of sequencing reads in each well. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacr_package",
   "language": "python",
   "name": "spacr_package"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
